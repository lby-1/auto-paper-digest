"""
å»é‡ç³»ç»Ÿæ¼”ç¤ºè„šæœ¬

å±•ç¤ºå»é‡ç³»ç»Ÿçš„å„ç§åŠŸèƒ½
"""

import sys
import io

# Fix encoding for Windows
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from apd.deduplicator import Deduplicator


def print_header(text):
    print("\n" + "=" * 70)
    print(f"  {text}")
    print("=" * 70)


def main():
    dedup = Deduplicator()

    print_header("æ™ºèƒ½å»é‡ç³»ç»Ÿæ¼”ç¤º")

    # Demo 1: arXiv IDæ ‡å‡†åŒ–
    print_header("Demo 1: arXiv IDæ ‡å‡†åŒ–")

    urls = [
        "https://arxiv.org/abs/2601.03252",
        "https://arxiv.org/pdf/2601.03252.pdf",
        "https://export.arxiv.org/pdf/2601.03252.pdf",
    ]

    print("\nä¸åŒæ ¼å¼çš„arXiv URL:")
    for url in urls:
        normalized = dedup.normalize_arxiv_id(url)
        print(f"  {url}")
        print(f"    â†’ {normalized}\n")

    # Demo 2: æ ‡é¢˜ç›¸ä¼¼åº¦
    print_header("Demo 2: æ ‡é¢˜ç›¸ä¼¼åº¦è®¡ç®—")

    title_pairs = [
        ("Attention Is All You Need", "Attention Is All You Need"),
        ("Attention Is All You Need", "Attention is all you need"),
        ("Attention Is All You Need", "Transformer: Attention Is All You Need"),
        ("Attention Is All You Need", "BERT: Pre-training Transformers"),
    ]

    print()
    for t1, t2 in title_pairs:
        similarity = dedup.compute_title_similarity(t1, t2)
        status = "âœ… ç›¸ä¼¼" if similarity >= 0.85 else "âŒ ä¸åŒ"
        print(f"æ ‡é¢˜1: {t1}")
        print(f"æ ‡é¢˜2: {t2}")
        print(f"ç›¸ä¼¼åº¦: {similarity:.2f} {status}\n")

    # Demo 3: å®Œæ•´å»é‡æµç¨‹
    print_header("Demo 3: å®Œæ•´å»é‡æµç¨‹")

    # æ¨¡æ‹Ÿè®ºæ–‡æ•°æ®
    papers = [
        {
            'paper_id': 'arxiv:2601.03252_hf',
            'title': 'Attention Is All You Need',
            'pdf_url': 'https://arxiv.org/pdf/2601.03252.pdf',
            'hf_url': 'https://huggingface.co/papers/2601.03252',
            'abstract': 'The dominant sequence transduction models...'
        },
        {
            'paper_id': 'arxiv:2601.03252_direct',
            'title': 'Attention Mechanism Research',
            'pdf_url': 'https://export.arxiv.org/pdf/2601.03252.pdf',  # åŒä¸€arXiv ID
            'hf_url': '',
            'abstract': 'Research on attention mechanisms...'
        },
        {
            'paper_id': 'paper_similar_title',
            'title': 'attention is all you need',  # ç›¸åŒæ ‡é¢˜ï¼Œä¸åŒå¤§å°å†™
            'pdf_url': '',
            'hf_url': '',
            'abstract': ''
        },
        {
            'paper_id': 'paper_unique_1',
            'title': 'BERT: Pre-training of Deep Bidirectional Transformers',
            'pdf_url': 'https://arxiv.org/pdf/1810.04805.pdf',
            'hf_url': '',
            'abstract': ''
        },
        {
            'paper_id': 'paper_unique_2',
            'title': 'GPT-3: Language Models are Few-Shot Learners',
            'pdf_url': 'https://arxiv.org/pdf/2005.14165.pdf',
            'hf_url': '',
            'abstract': ''
        },
    ]

    print(f"\nè¾“å…¥: {len(papers)} ç¯‡è®ºæ–‡")
    for paper in papers:
        print(f"  - {paper['paper_id']}: {paper['title'][:50]}")

    print("\n\nğŸ” è¿è¡Œå»é‡æ£€æµ‹...")
    result = dedup.find_duplicates(papers, use_semantic=False)

    print(f"\nğŸ“Š å»é‡ç»“æœ:")
    print(f"  æ€»è®ºæ–‡æ•°: {result.total_papers}")
    print(f"  å”¯ä¸€è®ºæ–‡: {result.unique_papers}")
    print(f"  é‡å¤ç»„æ•°: {len(result.duplicate_groups)}")
    print(f"  å»é™¤é‡å¤: {result.duplicates_removed}")
    print(f"  å»é‡ç‡: {result.duplicates_removed/result.total_papers*100:.1f}%")

    if result.duplicate_groups:
        print(f"\nğŸ“‹ é‡å¤ç»„è¯¦æƒ…:")
        for i, group in enumerate(result.duplicate_groups, 1):
            print(f"\n  ç»„ {i}: æ£€æµ‹æ–¹æ³•={group.detection_method}")
            print(f"    ä¸»è®ºæ–‡: {group.canonical_paper_id}")
            print(f"    é‡å¤é¡¹:")
            for dup_id in group.duplicate_paper_ids:
                score = group.similarity_scores.get(dup_id, 0.0)
                print(f"      - {dup_id} (ç›¸ä¼¼åº¦: {score:.2f})")

    # Demo 4: ç»Ÿè®¡ä¿¡æ¯
    print_header("Demo 4: ç»Ÿè®¡ä¿¡æ¯")

    stats = dedup.get_deduplication_stats(result)

    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»è®ºæ–‡æ•°: {stats['total_papers']}")
    print(f"  å”¯ä¸€è®ºæ–‡: {stats['unique_papers']}")
    print(f"  é‡å¤ç»„æ•°: {stats['duplicate_groups']}")
    print(f"  å»é™¤é‡å¤: {stats['duplicates_removed']}")
    print(f"  å»é‡ç‡: {stats['deduplication_rate']*100:.1f}%")

    print(f"\næ£€æµ‹æ–¹æ³•åˆ†å¸ƒ:")
    print(f"  ç²¾ç¡®URLåŒ¹é…: {stats['detection_methods']['exact_url']}")
    print(f"  æ ‡é¢˜ç›¸ä¼¼åº¦: {stats['detection_methods']['title_similarity']}")
    print(f"  è¯­ä¹‰ç›¸ä¼¼åº¦: {stats['detection_methods']['semantic_similarity']}")

    # æ€»ç»“
    print_header("èµ„æºèŠ‚çœä¼°ç®—")

    saved_papers = result.duplicates_removed
    time_per_paper = 15  # åˆ†é’Ÿ
    cost_per_paper = 0.5  # ç¾å…ƒ

    print(f"\nå¦‚æœæ¯ç¯‡è®ºæ–‡å¤„ç†éœ€è¦:")
    print(f"  - æ—¶é—´: {time_per_paper} åˆ†é’Ÿ")
    print(f"  - æˆæœ¬: ${cost_per_paper}")

    print(f"\né€šè¿‡å»é‡èŠ‚çœ:")
    print(f"  - æ—¶é—´: {saved_papers * time_per_paper} åˆ†é’Ÿ = {saved_papers * time_per_paper / 60:.1f} å°æ—¶")
    print(f"  - æˆæœ¬: ${saved_papers * cost_per_paper}")
    print(f"  - å­˜å‚¨ç©ºé—´: ~{saved_papers * 100} MB")

    print("\n" + "=" * 70)
    print("  æ¼”ç¤ºå®Œæˆ!")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()
