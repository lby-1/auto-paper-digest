"""
çœŸå®åœºæ™¯å»é‡æµ‹è¯•

æ¨¡æ‹Ÿå®é™…ä½¿ç”¨ä¸­å¯èƒ½é‡åˆ°çš„é‡å¤åœºæ™¯
"""

import sys
import io

# Fix encoding for Windows
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from apd.db import upsert_paper, get_connection
from apd.deduplicator import Deduplicator
from apd.utils import now_iso

def setup_test_data():
    """è®¾ç½®æµ‹è¯•æ•°æ®ï¼šåˆ›å»ºä¸€äº›å…·æœ‰é‡å¤ç‰¹å¾çš„è®ºæ–‡"""

    week_id = "2026-05"

    # åœºæ™¯1: åŒä¸€ç¯‡è®ºæ–‡ä»ä¸åŒæ¥æºè·å–ï¼ˆarXiv vs HuggingFaceï¼‰
    print("ğŸ“ åœºæ™¯1: åŒä¸€è®ºæ–‡çš„ä¸åŒæ¥æº")
    upsert_paper(
        paper_id="test_dup_arxiv_1",
        week_id=week_id,
        title="Attention Is All You Need",
        pdf_url="https://arxiv.org/pdf/1706.03762.pdf",
        hf_url="",
        content_type="PAPER",
        summary="The dominant sequence transduction models..."
    )

    upsert_paper(
        paper_id="test_dup_hf_1",
        week_id=week_id,
        title="Attention Is All You Need",
        pdf_url="https://export.arxiv.org/pdf/1706.03762.pdf",  # åŒä¸€arXiv IDï¼Œä¸åŒåŸŸå
        hf_url="https://huggingface.co/papers/1706.03762",
        content_type="PAPER",
        summary="The dominant sequence transduction models..."
    )
    print("  âœ“ æ·»åŠ äº†2ç¯‡ç›¸åŒè®ºæ–‡ï¼ˆä¸åŒURLæ ¼å¼ï¼‰")

    # åœºæ™¯2: æ ‡é¢˜ç•¥æœ‰ä¸åŒä½†å®é™…æ˜¯åŒä¸€è®ºæ–‡
    print("\nğŸ“ åœºæ™¯2: æ ‡é¢˜å¤§å°å†™å’Œæ ‡ç‚¹å·®å¼‚")
    upsert_paper(
        paper_id="test_dup_case_1",
        week_id=week_id,
        title="BERT: Pre-training of Deep Bidirectional Transformers",
        pdf_url="https://arxiv.org/pdf/1810.04805.pdf",
        hf_url="",
        content_type="PAPER",
        summary="We introduce BERT..."
    )

    upsert_paper(
        paper_id="test_dup_case_2",
        week_id=week_id,
        title="bert pre-training of deep bidirectional transformers",  # å…¨å°å†™
        pdf_url="",
        hf_url="https://huggingface.co/papers/1810.04805",
        content_type="PAPER",
        summary="We introduce BERT..."
    )
    print("  âœ“ æ·»åŠ äº†2ç¯‡ç›¸åŒè®ºæ–‡ï¼ˆä¸åŒå¤§å°å†™ï¼‰")

    # åœºæ™¯3: æ ‡é¢˜æœ‰å‰ç¼€/åç¼€çš„é‡å¤
    print("\nğŸ“ åœºæ™¯3: æ ‡é¢˜æœ‰é¢å¤–å‰ç¼€")
    upsert_paper(
        paper_id="test_dup_prefix_1",
        week_id=week_id,
        title="GPT-3: Language Models are Few-Shot Learners",
        pdf_url="https://arxiv.org/pdf/2005.14165.pdf",
        hf_url="",
        content_type="PAPER",
        summary="Recent work has demonstrated..."
    )

    upsert_paper(
        paper_id="test_dup_prefix_2",
        week_id=week_id,
        title="[2020] GPT-3: Language Models are Few-Shot Learners",  # å¸¦å¹´ä»½å‰ç¼€
        pdf_url="",
        hf_url="https://huggingface.co/papers/2005.14165",
        content_type="PAPER",
        summary="Recent work has demonstrated..."
    )
    print("  âœ“ æ·»åŠ äº†2ç¯‡ç›¸åŒè®ºæ–‡ï¼ˆä¸€ä¸ªæœ‰å¹´ä»½å‰ç¼€ï¼‰")

    # åœºæ™¯4: å®Œå…¨ä¸åŒçš„è®ºæ–‡ï¼ˆä¸åº”è¯¥è¢«æ ‡è®°ä¸ºé‡å¤ï¼‰
    print("\nğŸ“ åœºæ™¯4: ä¸åŒçš„è®ºæ–‡ï¼ˆå¯¹ç…§ç»„ï¼‰")
    upsert_paper(
        paper_id="test_unique_1",
        week_id=week_id,
        title="ResNet: Deep Residual Learning for Image Recognition",
        pdf_url="https://arxiv.org/pdf/1512.03385.pdf",
        hf_url="",
        content_type="PAPER",
        summary="Deeper neural networks are more difficult to train..."
    )

    upsert_paper(
        paper_id="test_unique_2",
        week_id=week_id,
        title="AlexNet: ImageNet Classification with Deep CNNs",
        pdf_url="https://arxiv.org/pdf/1404.5997.pdf",
        hf_url="",
        content_type="PAPER",
        summary="We trained a large, deep convolutional neural network..."
    )
    print("  âœ“ æ·»åŠ äº†2ç¯‡ä¸åŒçš„è®ºæ–‡")

    print(f"\nâœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆï¼šå…±8ç¯‡è®ºæ–‡ï¼Œé¢„æœŸæ£€æµ‹åˆ°3ç»„é‡å¤ï¼ˆ6ç¯‡é‡å¤è®ºæ–‡ï¼‰")


def run_dedup_test():
    """è¿è¡Œå»é‡æµ‹è¯•"""

    print("\n" + "="*70)
    print("  å¼€å§‹å»é‡æ£€æµ‹")
    print("="*70)

    # è·å–æ‰€æœ‰æµ‹è¯•è®ºæ–‡
    with get_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT paper_id, title, pdf_url, hf_url, summary
            FROM papers
            WHERE paper_id LIKE 'test_%'
            ORDER BY updated_at DESC
        """)
        rows = cursor.fetchall()
        papers = [dict(row) for row in rows]

    print(f"\nğŸ“Š è¾“å…¥æ•°æ®ï¼š{len(papers)} ç¯‡è®ºæ–‡")
    for i, paper in enumerate(papers, 1):
        print(f"  {i}. {paper['paper_id']}: {paper['title'][:60]}...")

    # è¿è¡Œå»é‡
    dedup = Deduplicator()
    result = dedup.find_duplicates(papers, use_semantic=False)

    print(f"\n" + "="*70)
    print("  å»é‡ç»“æœ")
    print("="*70)

    print(f"\nğŸ“ˆ ç»Ÿè®¡æ•°æ®ï¼š")
    print(f"  æ€»è®ºæ–‡æ•°ï¼š{result.total_papers}")
    print(f"  å”¯ä¸€è®ºæ–‡ï¼š{result.unique_papers}")
    print(f"  é‡å¤ç»„æ•°ï¼š{len(result.duplicate_groups)}")
    print(f"  æ£€æµ‹åˆ°çš„é‡å¤ï¼š{result.duplicates_removed}")
    print(f"  å»é‡ç‡ï¼š{result.duplicates_removed/result.total_papers*100:.1f}%")

    if result.duplicate_groups:
        print(f"\nğŸ“‹ é‡å¤ç»„è¯¦æƒ…ï¼š")
        for i, group in enumerate(result.duplicate_groups, 1):
            print(f"\n  ç»„ {i}: æ£€æµ‹æ–¹æ³•={group.detection_method}")
            print(f"    ä¸»è®ºæ–‡ï¼š{group.canonical_paper_id}")
            print(f"    æ ‡é¢˜ï¼š", end="")
            # è·å–ä¸»è®ºæ–‡æ ‡é¢˜
            main_paper = next((p for p in papers if p['paper_id'] == group.canonical_paper_id), None)
            if main_paper:
                print(f"{main_paper['title']}")
            print(f"    é‡å¤é¡¹ï¼š")
            for dup_id in group.duplicate_paper_ids:
                score = group.similarity_scores.get(dup_id, 0.0)
                dup_paper = next((p for p in papers if p['paper_id'] == dup_id), None)
                if dup_paper:
                    print(f"      - {dup_id} (ç›¸ä¼¼åº¦: {score:.2f})")
                    print(f"        {dup_paper['title']}")

    # è®¡ç®—èµ„æºèŠ‚çœ
    print(f"\n" + "="*70)
    print("  èµ„æºèŠ‚çœä¼°ç®—")
    print("="*70)

    saved_papers = result.duplicates_removed
    time_per_paper = 15  # åˆ†é’Ÿ
    cost_per_paper = 0.5  # ç¾å…ƒ
    storage_per_paper = 100  # MB

    print(f"\nğŸ’° é€šè¿‡å»é‡èŠ‚çœï¼š")
    print(f"  â±ï¸  å¤„ç†æ—¶é—´ï¼š{saved_papers * time_per_paper} åˆ†é’Ÿ = {saved_papers * time_per_paper / 60:.1f} å°æ—¶")
    print(f"  ğŸ’µ å¤„ç†æˆæœ¬ï¼š${saved_papers * cost_per_paper:.2f}")
    print(f"  ğŸ’¾ å­˜å‚¨ç©ºé—´ï¼š~{saved_papers * storage_per_paper} MB")

    if result.total_papers > 0:
        print(f"\nğŸ“Š æ•ˆç‡æå‡ï¼š")
        print(f"  å¤„ç†æ•ˆç‡æå‡ï¼š{(1 - result.unique_papers/result.total_papers)*100:.1f}%")
        print(f"  æˆæœ¬é™ä½ï¼š{(saved_papers * cost_per_paper) / (result.total_papers * cost_per_paper) * 100:.1f}%")


def cleanup_test_data():
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    print(f"\n" + "="*70)
    print("  æ¸…ç†æµ‹è¯•æ•°æ®")
    print("="*70)

    with get_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("DELETE FROM papers WHERE paper_id LIKE 'test_%'")
        deleted = cursor.rowcount
        conn.commit()

    print(f"âœ… å·²åˆ é™¤ {deleted} æ¡æµ‹è¯•è®°å½•")


if __name__ == "__main__":
    print("\n" + "="*70)
    print("  çœŸå®åœºæ™¯å»é‡ç³»ç»Ÿæµ‹è¯•")
    print("="*70 + "\n")

    try:
        # è®¾ç½®æµ‹è¯•æ•°æ®
        setup_test_data()

        # è¿è¡Œå»é‡æµ‹è¯•
        run_dedup_test()

    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        cleanup_test_data()

    print("\n" + "="*70)
    print("  æµ‹è¯•å®Œæˆï¼")
    print("="*70 + "\n")
