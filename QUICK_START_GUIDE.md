# å¿«é€Ÿå®æ–½æŒ‡å— - Quick Start

> æœ¬æ–‡æ¡£æ˜¯ `OPTIMIZATION_ROADMAP.md` çš„ç²¾ç®€ç‰ˆï¼Œæä¾›æœ€å¿«ä¸Šæ‰‹çš„ä¼˜åŒ–å®æ–½æ–¹æ¡ˆã€‚

---

## ğŸš€ ç«‹å³å¯åšï¼ˆ1-2å¤©ï¼‰

### 1. æ·»åŠ é…ç½®æ–‡ä»¶æ‰©å±•

```bash
# .env æ·»åŠ æ–°é…ç½®
cat >> .env << 'EOF'

# === æ–°å¢é…ç½® ===

# è´¨é‡æ§åˆ¶
QUALITY_THRESHOLD=60.0
MIN_CITATIONS=0
MIN_GITHUB_STARS=100

# TTSé…ç½®
DEFAULT_TTS_ENGINE=notebooklm
ENABLE_EDGE_TTS_FALLBACK=true

# å‘å¸ƒé…ç½®
AUTO_PUBLISH=false
ENABLE_SEMI_AUTO_MODE=true
EOF
```

### 2. å®‰è£…é¢å¤–ä¾èµ–

```bash
# å»é‡ä¸è´¨é‡è¿‡æ»¤
pip install sentence-transformers scikit-learn python-Levenshtein

# æ•°æ®åˆ†æ
pip install plotly pandas

# å¤‡ç”¨TTS
pip install edge-tts

# arXivç›´æ¥é›†æˆ
pip install arxiv
```

---

## ğŸ“Š Week 1: è´¨é‡æ§åˆ¶ï¼ˆæ¨èä¼˜å…ˆï¼‰

### ä»»åŠ¡æ¸…å•

- [ ] **Day 1-2:** å®ç°ç®€å•çš„æ ‡é¢˜å»é‡
  ```python
  # apd/simple_dedup.py
  def find_duplicate_titles(papers: list[Paper]) -> list[tuple]:
      """ç®€å•çš„æ ‡é¢˜å»é‡ï¼ˆç¼–è¾‘è·ç¦»ï¼‰"""
      from difflib import SequenceMatcher

      duplicates = []
      for i, p1 in enumerate(papers):
          for p2 in papers[i+1:]:
              ratio = SequenceMatcher(None, p1.title, p2.title).ratio()
              if ratio > 0.85:
                  duplicates.append((p1, p2, ratio))
      return duplicates
  ```

- [ ] **Day 3-4:** é›†æˆSemantic Scholar APIè·å–å¼•ç”¨æ•°
  ```python
  # apd/s2_api.py
  import requests

  def get_citation_count(arxiv_id: str) -> int:
      """è·å–è®ºæ–‡å¼•ç”¨æ•°"""
      url = f"https://api.semanticscholar.org/graph/v1/paper/arXiv:{arxiv_id}"
      response = requests.get(url, params={'fields': 'citationCount'})
      if response.ok:
          return response.json().get('citationCount', 0)
      return 0
  ```

- [ ] **Day 5:** æ·»åŠ è´¨é‡è¿‡æ»¤CLIå‘½ä»¤
  ```bash
  # ä½¿ç”¨ç¤ºä¾‹
  apd fetch --week 2026-W04 --min-citations 5 --max 20
  ```

**é¢„æœŸæ•ˆæœ:**
- âœ… è‡ªåŠ¨è¿‡æ»¤ä½è´¨é‡è®ºæ–‡
- âœ… å‡å°‘50%æ— æ•ˆè§†é¢‘ç”Ÿæˆ
- âœ… æå‡å†…å®¹æ•´ä½“è´¨é‡

---

## ğŸ“ˆ Week 2: æ•°æ®ç»Ÿè®¡ï¼ˆå¿«é€Ÿè§æ•ˆï¼‰

### ä»»åŠ¡æ¸…å•

- [ ] **Day 1-2:** æ•°æ®åº“æ‰©å±•
  ```sql
  -- æ‰§è¡ŒSQL
  ALTER TABLE papers ADD COLUMN views INTEGER DEFAULT 0;
  ALTER TABLE papers ADD COLUMN likes INTEGER DEFAULT 0;
  ALTER TABLE papers ADD COLUMN comments INTEGER DEFAULT 0;
  ALTER TABLE papers ADD COLUMN publish_time TIMESTAMP;
  ```

- [ ] **Day 3-4:** ç®€å•çš„æ•°æ®é‡‡é›†è„šæœ¬
  ```python
  # scripts/collect_metrics.py
  import re
  from playwright.sync_api import sync_playwright

  def get_douyin_stats(video_url: str) -> dict:
      """è·å–æŠ–éŸ³è§†é¢‘æ•°æ®"""
      with sync_playwright() as p:
          browser = p.chromium.launch()
          page = browser.new_page()
          page.goto(video_url)

          # æå–æ•°æ®ï¼ˆéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´ï¼‰
          likes = page.locator('[data-e2e="like-count"]').inner_text()

          return {'likes': int(likes.replace('w', '0000'))}
  ```

- [ ] **Day 5:** ç”Ÿæˆç®€å•æŠ¥å‘Š
  ```python
  # apd/simple_report.py
  def generate_weekly_report(week_id: str):
      """ç”Ÿæˆå‘¨æŠ¥å‘Š"""
      papers = db.get_papers_by_week(week_id)

      total_views = sum(p.views for p in papers)
      total_likes = sum(p.likes for p in papers)

      print(f"""
      === {week_id} å‘¨æŠ¥ ===
      å‘å¸ƒè§†é¢‘æ•°: {len(papers)}
      æ€»è§‚çœ‹é‡: {total_views:,}
      æ€»ç‚¹èµæ•°: {total_likes:,}
      å¹³å‡è§‚çœ‹: {total_views/len(papers):,.0f}
      """)
  ```

**ä½¿ç”¨æ–¹å¼:**
```bash
python scripts/collect_metrics.py --week 2026-W04
apd report --week 2026-W04
```

---

## ğŸ™ï¸ Week 3: TTSå¤‡é€‰æ–¹æ¡ˆï¼ˆé™ä½é£é™©ï¼‰

### ä»»åŠ¡æ¸…å•

- [ ] **Day 1:** é›†æˆEdge TTSï¼ˆå®Œå…¨å…è´¹ï¼‰
  ```python
  # apd/tts/edge_tts_engine.py
  import edge_tts
  import asyncio

  async def generate_audio(text: str, output_path: str):
      """ä½¿ç”¨Edge TTSç”Ÿæˆè¯­éŸ³"""
      communicate = edge_tts.Communicate(
          text=text,
          voice="zh-CN-XiaoxiaoNeural"
      )
      await communicate.save(output_path)

  # åŒæ­¥åŒ…è£…
  def synthesize(text: str, output_path: str):
      asyncio.run(generate_audio(text, output_path))
  ```

- [ ] **Day 2-3:** æ·»åŠ fallbacké€»è¾‘
  ```python
  # apd/nblm_bot.py ä¿®æ”¹
  def generate_video_with_fallback(paper: Paper):
      """å¸¦fallbackçš„è§†é¢‘ç”Ÿæˆ"""
      try:
          # å°è¯•NotebookLM
          return self.generate_via_notebooklm(paper)
      except Exception as e:
          logger.warning(f"NotebookLM failed: {e}, using Edge TTS")
          # Fallbackåˆ°Edge TTS
          return self.generate_via_edge_tts(paper)
  ```

- [ ] **Day 4-5:** æµ‹è¯•ä¸æ–‡æ¡£

**ä½¿ç”¨æ–¹å¼:**
```bash
# å¼ºåˆ¶ä½¿ç”¨Edge TTS
apd upload --week 2026-W04 --tts-engine edge

# è‡ªåŠ¨fallbackï¼ˆé»˜è®¤ï¼‰
apd upload --week 2026-W04
```

---

## ğŸ”Œ Week 4: æ‰©å±•å†…å®¹æºï¼ˆæå‡å†…å®¹ä¸°å¯Œåº¦ï¼‰

### ä»»åŠ¡æ¸…å•

- [ ] **Day 1-2:** arXiv APIç›´æ¥é›†æˆ
  ```python
  # apd/arxiv_fetcher.py
  import arxiv

  def fetch_recent_papers(categories=['cs.AI', 'cs.CL'], max_results=50):
      """ç›´æ¥ä»arXivè·å–è®ºæ–‡"""
      client = arxiv.Client()
      search = arxiv.Search(
          query=f"cat:{' OR cat:'.join(categories)}",
          max_results=max_results,
          sort_by=arxiv.SortCriterion.SubmittedDate
      )

      papers = []
      for result in client.results(search):
          papers.append({
              'title': result.title,
              'pdf_url': result.pdf_url,
              'abstract': result.summary,
              'arxiv_id': result.get_short_id(),
          })
      return papers
  ```

- [ ] **Day 3:** æ·»åŠ CLIå‘½ä»¤
  ```python
  # apd/cli.py
  @click.command()
  @click.option('--categories', default='cs.AI,cs.CL')
  @click.option('--max', default=50)
  def fetch_arxiv(categories, max):
      """ç›´æ¥ä»arXivè·å–è®ºæ–‡"""
      cats = categories.split(',')
      papers = fetch_recent_papers(cats, max)
      # ä¿å­˜åˆ°æ•°æ®åº“
      for p in papers:
          db.save_paper(p)
  ```

- [ ] **Day 4-5:** æµ‹è¯•ä¸ä¼˜åŒ–

**ä½¿ç”¨æ–¹å¼:**
```bash
apd fetch-arxiv --categories cs.AI,cs.LG --max 30
```

---

## ğŸ“± Month 2: å¹³å°æ‰©å±•

### YouTubeå‘å¸ƒï¼ˆæ¨èä¼˜å…ˆï¼‰

**å‡†å¤‡å·¥ä½œ:**
1. è·å–YouTube API credentials
2. å®‰è£…ä¾èµ–: `pip install google-api-python-client google-auth-oauthlib`

**å®æ–½æ­¥éª¤:**

```python
# apd/youtube_bot.py
from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow

class YouTubePublisher:
    SCOPES = ['https://www.googleapis.com/auth/youtube.upload']

    def __init__(self):
        flow = InstalledAppFlow.from_client_secrets_file(
            'client_secrets.json', self.SCOPES
        )
        credentials = flow.run_local_server()
        self.youtube = build('youtube', 'v3', credentials=credentials)

    def upload_video(self, video_path, title, description):
        """ä¸Šä¼ è§†é¢‘åˆ°YouTube"""
        from googleapiclient.http import MediaFileUpload

        body = {
            'snippet': {
                'title': title,
                'description': description,
                'categoryId': '28'  # Science & Technology
            },
            'status': {'privacyStatus': 'public'}
        }

        media = MediaFileUpload(video_path, resumable=True)
        request = self.youtube.videos().insert(
            part='snippet,status',
            body=body,
            media_body=media
        )

        response = request.execute()
        return f"https://youtube.com/watch?v={response['id']}"
```

**CLIé›†æˆ:**
```bash
apd publish-youtube --week 2026-W04
```

---

## ğŸ¯ ä¼˜å…ˆçº§å»ºè®®

æ ¹æ®ä½ çš„å…·ä½“æƒ…å†µé€‰æ‹©ï¼š

### åœºæ™¯1: å†…å®¹è´¨é‡ä¸ç¨³å®š
**ä¼˜å…ˆ:** Week 1 è´¨é‡æ§åˆ¶
- å®æ–½å»é‡ç³»ç»Ÿ
- æ·»åŠ å¼•ç”¨æ•°è¿‡æ»¤
- æ•ˆæœç«‹ç«¿è§å½±

### åœºæ™¯2: æƒ³äº†è§£å‘å¸ƒæ•ˆæœ
**ä¼˜å…ˆ:** Week 2 æ•°æ®ç»Ÿè®¡
- å¿«é€Ÿçœ‹åˆ°æ•°æ®
- æŒ‡å¯¼åç»­ä¼˜åŒ–
- æŠ€æœ¯éš¾åº¦ä½

### åœºæ™¯3: æ‹…å¿ƒNotebookLMç¨³å®šæ€§
**ä¼˜å…ˆ:** Week 3 TTSå¤‡é€‰
- Edge TTSå®Œå…¨å…è´¹
- é™ä½å•ç‚¹æ•…éšœé£é™©
- å®æ–½ç›¸å¯¹ç®€å•

### åœºæ™¯4: éœ€è¦æ›´å¤šå†…å®¹
**ä¼˜å…ˆ:** Week 4 å†…å®¹æºæ‰©å±•
- arXiv APIæ›´åŠæ—¶
- ä¸°å¯Œå†…å®¹ç±»å‹
- æå‡ç«äº‰åŠ›

---

## ğŸ“¦ å®Œæ•´å®æ–½åŒ…ï¼ˆAll-in-Oneï¼‰

å¦‚æœä½ æœ‰å……è¶³æ—¶é—´ï¼ŒæŒ‰æ­¤é¡ºåºå®æ–½æ•ˆæœæœ€ä½³ï¼š

```
Week 1: è´¨é‡æ§åˆ¶
  â†“ (è´¨é‡æå‡åå†æ‰©é‡)
Week 4: å†…å®¹æºæ‰©å±•
  â†“ (æœ‰æ›´å¤šå†…å®¹å)
Week 2: æ•°æ®ç»Ÿè®¡
  â†“ (åŸºäºæ•°æ®ä¼˜åŒ–)
Week 3: TTSå¤‡é€‰æ–¹æ¡ˆ
  â†“ (é™ä½é£é™©)
Month 2: å¹³å°æ‰©å±•
```

---

## ğŸ› ï¸ å¼€å‘å·¥å…·æ¨è

### ä»£ç è´¨é‡
```bash
# å®‰è£…
pip install black isort flake8 mypy

# ä½¿ç”¨
black apd/  # ä»£ç æ ¼å¼åŒ–
isort apd/  # importæ’åº
flake8 apd/  # ä»£ç æ£€æŸ¥
mypy apd/  # ç±»å‹æ£€æŸ¥
```

### æµ‹è¯•
```bash
pip install pytest pytest-cov

# è¿è¡Œæµ‹è¯•
pytest tests/

# è¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=apd --cov-report=html
```

### æ€§èƒ½åˆ†æ
```bash
pip install py-spy

# æ€§èƒ½åˆ†æ
py-spy top -- python -m apd.cli upload --week 2026-W04
```

---

## ğŸ“š å­¦ä¹ èµ„æº

### APIæ–‡æ¡£
- [Semantic Scholar API](https://api.semanticscholar.org/)
- [arXiv API](https://arxiv.org/help/api)
- [YouTube Data API](https://developers.google.com/youtube/v3)
- [Edge TTS](https://github.com/rany2/edge-tts)

### ç›¸å…³æŠ€æœ¯
- [Sentence Transformers](https://www.sbert.net/)
- [Playwright Python](https://playwright.dev/python/)
- [SQLite Documentation](https://www.sqlite.org/docs.html)

---

## â“ å¸¸è§é—®é¢˜

**Q: æˆ‘åº”è¯¥ä»å“ªä¸ªä¼˜åŒ–å¼€å§‹ï¼Ÿ**
A: å¦‚æœä¸ç¡®å®šï¼Œä» **Week 1 è´¨é‡æ§åˆ¶** å¼€å§‹ï¼Œæ•ˆæœæœ€æ˜æ˜¾ã€‚

**Q: è¿™äº›ä¼˜åŒ–ä¼šç ´åç°æœ‰åŠŸèƒ½å—ï¼Ÿ**
A: ä¸ä¼šã€‚æ‰€æœ‰ä¼˜åŒ–éƒ½æ˜¯å‘åå…¼å®¹çš„ï¼Œå¯ä»¥é€æ­¥å¯ç”¨ã€‚

**Q: éœ€è¦é¢å¤–çš„æœåŠ¡å™¨å—ï¼Ÿ**
A: å‰4å‘¨çš„ä¼˜åŒ–éƒ½ä¸éœ€è¦ã€‚Month 2çš„å¹³å°æ‰©å±•å¯èƒ½éœ€è¦äº‘æœåŠ¡å™¨ï¼ˆå¯é€‰ï¼‰ã€‚

**Q: ä¼°è®¡æ€»å¼€å‘æ—¶é—´ï¼Ÿ**
A:
- Week 1-4: æ¯å‘¨æŠ•å…¥20-30å°æ—¶ï¼Œå¯ç”±1äººå®Œæˆ
- Month 2: éœ€è¦40-60å°æ—¶ï¼Œå»ºè®®2äººåä½œ

**Q: æœ‰ç¤ºä¾‹ä»£ç å—ï¼Ÿ**
A: æœ¬æ–‡æ¡£ä¸­çš„ä»£ç éƒ½æ˜¯å¯è¿è¡Œçš„ç¤ºä¾‹ï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨ã€‚

---

## ğŸ¤ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿ
1. æŸ¥çœ‹ `OPTIMIZATION_ROADMAP.md` è¯¦ç»†æ–‡æ¡£
2. æŸ¥çœ‹ `COMPETITORS_ANALYSIS.md` äº†è§£æœ€ä½³å®è·µ
3. æäº¤ GitHub Issue
4. å‚ä¸ GitHub Discussions

---

**ç¥ä½ ä¼˜åŒ–é¡ºåˆ©ï¼ ğŸš€**

*æœ€åæ›´æ–°: 2026-01-23*
